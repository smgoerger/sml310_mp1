\documentclass{article}
\usepackage{graphicx}

\begin{document}

<<setup, echo = FALSE, message = FALSE, warning = FALSE, results = FALSE>>=
library(rstan)
library(dplyr)

options("scipen" = 100, "digits" = 3)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = FALSE, out.width = '5.5in')

@

\section*{Problem 1}
<<p1>>=
N <- c(10000, 100000,250000, 300000, 450000, 600000, 1000000, 1500000, 2000000, 2500000)
ps <- NA
for (i in 1:length(N)){
  e <- rnorm(N[i], 0, sd = 5)
  x <- rnorm(N[i], 0, sd = 1)
  y <- 3 + 0.01*x + e
  ps[i] <- summary(lm(y ~ x))$coefficients[2,4]
}



#hist(y)
plot(N, ps)
abline(h = .05)

#cor(x, y)
@


\section*{Problem 2}
<<p2>>=
N.p2 <- 1000
test_scores <- rnorm(N.p2, 21, sd = 5)
hist(test_scores)
undergrad_ivy <- rbinom(N.p2, size = 1, prob = plogis(test_scores - 21))
hist(undergrad_ivy)
grad_notIvy <- 1 - rbinom(N.p2, size = 1, prob = undergrad_ivy)
hist(grad_notIvy)

cor(test_scores, grad_notIvy)

summary(lm(grad_notIvy ~ undergrad_ivy + test_scores))
@


<<p2b>>=
N.p2 <- 1000
test_scores <- rnorm(N.p2, 21, sd = 5)
hist(test_scores)
undergrad_ivy <- rbinom(N.p2, size = 1, prob = plogis(test_scores - 21))
hist(undergrad_ivy)
grad_notIvy <- 1 - rbinom(N.p2, size = 1, prob = undergrad_ivy)
hist(grad_notIvy)

cor(test_scores, grad_notIvy)

summary(lm(grad_notIvy ~ undergrad_ivy + test_scores))
@
%Alter that to be more about ivy league university
%copy the code so they don't have to look for it


\section*{Problem 3}
% p3a
\begin{figure}[h!]
\includegraphics[scale=.1]{/Users/samanthagoerger/Desktop/sml310_mp1/p3a.jpg}
\end{figure}

% N = game number
% scored[N] = array of scores indexed by game
% attempted[N] = array of number of attemps indexed by game

% mu = overall average skill level
% sigma = overall stdev of skill level, lower bounded by 0

% alpha[N] = array of average skill level indexed by game
% loop samples from ~N(mu, sigma^2)

% alpha_norm ~ N(0, 1)
% scored sum of bernoulli of attempted using prob of alpha(skill level)

<<p3b>>=
lines <- 
"Game   Scored  N.Attempts
1   4   5
2   5   11
3   5   14
4   5   12
5   2   7
6   7   10
7   6   14
8   9   15
9   4   12
10  1   4
11  13  27
12  5   17
13  6   12
14  9   9
15  7   12
16  3   10
17  8   12
18  1   6
19  18  39
20  3   13
21  10  17
22  1   6
23  3   12"
con <- textConnection(lines)
shaq <- read.csv(con, sep="")
shaq


shaq_model_stan <- "
data{
  int<lower=0> N; 
  int scored[N];
  int attempted[N];
}

parameters{
  real mu;
  real<lower=0> sigma;
  vector[N] alpha_norm;
}

transformed parameters{
  real alpha[N];
  for(n in 1:N)
    alpha[n] = mu + sigma * alpha_norm[n];
}

model{
  alpha_norm ~ normal(0, 1);
  scored ~ binomial(attempted, inv_logit(alpha));
}"

adaptSteps = 1000            # Number of steps to "tune" the samplers.
burnInSteps = 5000           # Number of steps to "burn-in" the samplers.
nChains = 3                  # Number of chains to run.
numSavedSteps=12000          # Total number of steps in chains to save.
thinSteps=10                 # Number of steps to "thin" (1=keep every step).

shaq_model <- stan_model(model_code = shaq_model_stan, model_name = "shaq_model")
shaq_fit <- sampling(object=shaq_model,
                      data = list(N=nrow(shaq), scored=shaq$Scored, attempted = shaq$N.Attempts),
                      chains = nChains ,
                      iter = ( ceiling(numSavedSteps/nChains)*thinSteps
                               +burnInSteps ) , 
                      warmup = burnInSteps , 
                      thin = thinSteps ,
                      init = "random" ) 

samples <- extract(shaq_fit)
samples_dat <- as.data.frame(samples)
@

<<p3bHist>>=
hist(samples$mu)
hist(samples$sigma)
hist(samples$alpha_norm)
# mostly doesn't deviate too much, the histogram of alpha norm
@

<<notes>>=
# partial pooling: each alpha affected by overall alpha
# parameters{
#  real mu;
#  real<lower=0> sigma;
#  vector[N] alpha_norm;
# }
#
# complete pooling: each alpha the same
# parameters{
#  real mu;
#  real<lower=0> sigma; // SIGMA SHOULD BE 0 //
#  vector[N] alpha_norm;
# }
# 
# no pooling: each alpha the same
# parameters{
#  real mu;
#  real<lower=0> sigma; // SIGMA SHOULD BE infinite in theory? //
#  vector[N] alpha_norm;
# }
@


<<p3cComplete>>=
shaq_model_stanComplete <- "
data{
  int<lower=0> N; 
  int scored[N];
  int attempted[N];
}

parameters{
  real mu;
  real sigma;
  vector[N] alpha_norm;
}

transformed parameters{
  sigma = 0;
  real alpha[N];
  for(n in 1:N)
    alpha[n] = mu + sigma * alpha_norm[n];
}

model{
  alpha_norm ~ normal(0, 1);
  scored ~ binomial(attempted, inv_logit(alpha));
}"

adaptSteps = 1000            # Number of steps to "tune" the samplers.
burnInSteps = 5000           # Number of steps to "burn-in" the samplers.
nChains = 3                  # Number of chains to run.
numSavedSteps=12000          # Total number of steps in chains to save.
thinSteps=10                 # Number of steps to "thin" (1=keep every step).

shaq_model <- stan_model(model_code = shaq_model_stanComplete, model_name = "shaq_model")
shaq_fit <- sampling(object=shaq_model,
                      data = list(N=nrow(shaq), scored=shaq$Scored, attempted = shaq$N.Attempts),
                      chains = nChains ,
                      iter = ( ceiling(numSavedSteps/nChains)*thinSteps
                               +burnInSteps ) , 
                      warmup = burnInSteps , 
                      thin = thinSteps ,
                      init = "random" ) 

samples <- extract(shaq_fit)
samples_dat <- as.data.frame(samples)
@

<<p3cCompleteHist>>=
hist(samples$mu)
hist(samples$sigma)
hist(samples$alpha_norm)
@

\end{document}

