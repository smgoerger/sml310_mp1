\documentclass{article}
\usepackage{graphicx}

\begin{document}

<<setup, echo = FALSE, message = FALSE, warning = FALSE, results = FALSE>>=
library(rstan)
library(dplyr)
library(tigerstats)
setwd("~/Desktop")

options("scipen" = 100, "digits" = 3)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = FALSE, out.width = '5.5in')

@

\section*{Problem 1}

<<p1>>=
set.seed(08544)
# N <- c(10000, 100000, 250000, 300000, 450000, 600000, 1000000, 1500000, 2000000, 2500000)
N <- c(10000, 100000, 250000, 300000, 450000, 600000)

ps <- c()
x <- c()
meanx <- c()
meany <- c()
y <- c()
for (i in 1:length(N)){
  e <- rnorm(N[i], 0, sd = 0.01)
  x <- rnorm(N[i], 0, sd = 0.01)
  y <- 3 + 0.01*x + e
  meanx <- c(meanx, mean(x))
  meany <- c(meany, mean(y))
  summary(lm(y ~ x))
  ps[i] <- summary(lm(y ~ x))$coefficients[2,4]
}

cor(x,y)
plot(x[1:100], y[1:100],
     main = "Weak relationship between x and y",
     xlab = "100 x",
     ylab = '100 y')

#hist(y)
plot(N, ps)
abline(h = .05)


#cor(x, y)
#N[ps < 0.05]
@

\noindent We set the parameters to $y = 3 + 0.01*x + e$, for which y has a weak relationship with x.  The correlation is \Sexpr{cor(x, y)}. We then ran regressions on samples of increasing size, from \Sexpr{N[1]} to \Sexpr{N[10]} and plotted the p-values for each regression.  As seen in the figure, the relationship is consistently statistically significant after 1000000 samples, which shows that as sample size increases, even weak relationships will be shown as significant.


\section*{Problem 2}

<<p2a>>=
set.seed(08544)
N.p2 <- 1000
#x1 is asian
x1 <- rbinom(N.p2, size = 1, prob = .056)
#hist(x1)
#personality "personality"
x2 <- rnorm(N.p2, (5 - x1*3), 1.5)
#hist(x2)
#not get into harvard?
y <- plogis(x2 - 6)
#hist(y)

summary(lm(y ~ x2 + x1))
cor(x1, y)
@
\noindent We completed Problem 2b prior to 2a, which is why the code is nearly identical. The correlation between x1 and y is \Sexpr{cor(x1, y)}, while the coefficient in the regression is \Sexpr{summary(lm(y ~ x2 + x1))$coefficients[3, 1]}, showing that the lurking variable x2 changes the sign.\\

<<p2b>>=
set.seed(08544)
N.p2 <- 1000
#asian is asian
asian <- rbinom(N.p2, size = 1, prob = .056)
#hist(asian)
#personality "personality"
personality <- rnorm(N.p2, (5 - asian*3), 1.5)
#hist(personality)
#not get into harvard?
harvard <- plogis(personality - 6)
#hist(harvard)

summary(lm(harvard ~ personality + asian))
cor(asian, harvard)
@

\noindent We chose to comment on the Harvard Circut Court decision on admission of Asian American applicants.  Here, x1 is binomially distributed to mark if the candidate is Asian (5.6\% of the U.S. population is Asian).  x2 is the ``personality'' score as described in the brief, which here is lowered if the applicant is Asian. y is binomially distributed for acceptance to Harvard based on the personality score distribution, which is lower for Asian applicants.  

\section*{Problem 3}
\subsection*{Part a}
\begin{figure}[h!]
% \includegraphics[scale=.1]{/sml310_mp1/p3a.jpg}
\end{figure}

\subsection*{Part b}

% N = game number
% scored[N] = array of scores indexed by game
% attempted[N] = array of number of attemps indexed by game

% mu = overall average skill level
% sigma = overall stdev of skill level, lower bounded by 0

% alpha[N] = array of average skill level indexed by game
% loop samples from ~N(mu, sigma^2)

% alpha_norm ~ N(0, 1)
% scored sum of bernoulli of attempted using prob of alpha(skill level)

<<p3b>>=
lines <- 
"Game   Scored  N.Attempts
1   4   5
2   5   11
3   5   14
4   5   12
5   2   7
6   7   10
7   6   14
8   9   15
9   4   12
10  1   4
11  13  27
12  5   17
13  6   12
14  9   9
15  7   12
16  3   10
17  8   12
18  1   6
19  18  39
20  3   13
21  10  17
22  1   6
23  3   12"
con <- textConnection(lines)
shaq <- read.csv(con, sep="")
shaq

# partial pooling: each alpha affected by overall alpha
shaq_model_stan <- "
data{
  // data we supply
  int<lower=0> N; // game number, must be positive
  int scored[N]; // array of scores indexed by game
  int attempted[N]; // array of number of attempts indexed by games
}

parameters{
  real mu; // overall average skill level
  real<lower=0> sigma; // overall stdev of skill level, lower bounded by 0
  vector[N] alpha_norm; // array of 'zscores'
}

transformed parameters{
  real alpha[N]; // array of average skill level indexed by game
                 // produced using alpha_norms
  for(n in 1:N)
    alpha[n] = mu + sigma * alpha_norm[n];
}

model{
  alpha_norm ~ normal(0, 1);
  scored ~ binomial(attempted, inv_logit(alpha)); // scored modeled using num attempted & 0 - 1 of alpha
}"

adaptSteps = 1000            # Number of steps to "tune" the samplers.
burnInSteps = 5000           # Number of steps to "burn-in" the samplers.
nChains = 3                  # Number of chains to run.
numSavedSteps=12000          # Total number of steps in chains to save.
thinSteps=10                 # Number of steps to "thin" (1=keep every step).

shaq_model <- stan_model(model_code = shaq_model_stan, model_name = "shaq_model")
shaq_fit <- sampling(object=shaq_model,
                      data = list(N=nrow(shaq), scored=shaq$Scored, attempted = shaq$N.Attempts),
                      chains = nChains,
                      iter = (ceiling(numSavedSteps/nChains)*thinSteps
                               + burnInSteps), 
                      warmup = burnInSteps, 
                      thin = thinSteps,
                      init = "random") 

samplesPartial <- extract(shaq_fit)

# posterior distribution for each parameter
hist(samplesPartial$mu) # prob distribution of mu
hist(samplesPartial$sigma) # prob distribution of sigma
hist(samplesPartial$alpha[, 1]) # prob distribution of alpha_1
hist(samplesPartial$alpha[, 2]) # prob distribution of alpha_2
# 95% of data lies within 2 stdev of the mean 0 --> 
good <- plogis(mean(samplesPartial$mu) + 2 * mean(samplesPartial$sigma))
bad <- plogis(mean(samplesPartial$mu) - 2 * mean(samplesPartial$sigma))
good
bad
@

<<p3bHist>>=
p.3og.mu <- hist(samplesPartial$mu)
p.3og.sigma <- hist(samplesPartial$sigma)
p.3og.alpha <- hist(samplesPartial$alpha)
# par(p.3og.mu, p.3og.sigma, p.3og.alpha)
@

<<p3cComplete, include=FALSE, echo= FALSE, eval = FALSE>>=
# complete pooling: each alpha is mu (sigma == 0)
shaq_model_stan <- "
data{
  // data we supply
  int<lower=0> N; // game number, must be positive
  int scored[N]; // array of scores indexed by game
  int attempted[N]; // array of number of attempts indexed by games
}

parameters{
  real mu; // overall average skill level
  vector[N] alpha_norm; // array of 'zscores'
}

transformed parameters{
  real alpha[N]; // array of average skill level indexed by game
                 // produced using alpha_norms
  for(n in 1:N)
    alpha[n] = mu + 0 * alpha_norm[n];
}

model{
  alpha_norm ~ normal(0, 1);
  scored ~ binomial(attempted, inv_logit(alpha)); // scored modeled using num attempted & 0 - 1 of alpha
}"

adaptSteps = 1000            # Number of steps to "tune" the samplers.
burnInSteps = 5000           # Number of steps to "burn-in" the samplers.
nChains = 3                  # Number of chains to run.
numSavedSteps=12000          # Total number of steps in chains to save.
thinSteps=10                 # Number of steps to "thin" (1=keep every step).

shaq_model <- stan_model(model_code = shaq_model_stan, model_name = "shaq_model")
shaq_fit <- sampling(object=shaq_model,
                      data = list(N=nrow(shaq), scored=shaq$Scored, attempted = shaq$N.Attempts),
                      chains = nChains ,
                      iter = ( ceiling(numSavedSteps/nChains)*thinSteps
                               +burnInSteps ) , 
                      warmup = burnInSteps , 
                      thin = thinSteps ,
                      init = "random" ) 

samplesComplete <- extract(shaq_fit)
@

<<p3cCompleteHist>>=
hist(samplesComplete$mu)
# plot an uniform dist of 0 for sigma
hist(samplesComplete$alpha[, 1])
hist(samplesComplete$alpha[, 2]) # they're all the same
@

<<p3cNo, include=FALSE, echo= FALSE, eval = FALSE>>=
# no pooling: each alpha random (sigma == inf or 99)
shaq_model_stan <- "
data{
  int<lower=0> N; 
  int scored[N];
  int attempted[N];
}

parameters{
  real mu;
  vector[N] alpha_norm;
}

transformed parameters{
  real alpha[N];
  for(n in 1:N)
    alpha[n] = mu + 20 * alpha_norm[n];
}

model{
  alpha_norm ~ normal(0, 1);
  scored ~ binomial(attempted, inv_logit(alpha));
}"

adaptSteps = 1000            # Number of steps to "tune" the samplers.
burnInSteps = 5000           # Number of steps to "burn-in" the samplers.
nChains = 3                  # Number of chains to run.
numSavedSteps=12000          # Total number of steps in chains to save.
thinSteps=10                 # Number of steps to "thin" (1=keep every step).

shaq_model <- stan_model(model_code = shaq_model_stan, model_name = "shaq_model")
shaq_fit <- sampling(object=shaq_model,
                      data = list(N=nrow(shaq), scored=shaq$Scored, attempted = shaq$N.Attempts),
                      chains = nChains ,
                      iter = ( ceiling(numSavedSteps/nChains)*thinSteps
                               +burnInSteps ) , 
                      warmup = burnInSteps , 
                      thin = thinSteps ,
                      init = "random" ) 

samplesNo <- extract(shaq_fit)
@

<<p3cNoHist>>=
hist(samplesNo$mu)
# plot an uniform dist of 99 for sigma
hist(samplesNo$alpha[, 1])
hist(samplesNo$alpha[, 2]) # they're all different
@
\end{document}

